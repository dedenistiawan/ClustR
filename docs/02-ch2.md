# Pemrosesan Awal Data {#pad}

```{=html}
<style>
body{
text-align: justify}
</style>
```












Dalam **data mining** dan **analisis klaster**, pemrosesan data awal merupakan tahap yang paling penting dan sering kali paling memakan waktu. Berdasarkan pengalaman praktis dan berbagai studi, sekitar **80% dari keseluruhan waktu analisis** dihabiskan untuk tahap ini, sementara hanya **20% sisanya** digunakan untuk penerapan algoritma dan interpretasi hasil. Hal ini terjadi karena data mentah sering kali tidak siap langsung digunakan dalam analisis dan memerlukan berbagai tahapan pembersihan, transformasi, dan reduksi sebelum bisa diolah dengan algoritma klasterisasi.

**Mengapa Pemrosesan Data Awal Memakan Waktu Lama?**

1.  **Data Cleaning (Pembersihan Data)**\
    Data yang dikumpulkan sering kali mengandung **kesalahan, nilai yang hilang, duplikasi, dan inkonsistensi format**. Proses pembersihan data mencakup penanganan **missing values**, deteksi **outlier**, serta standarisasi format data agar seragam. Setiap langkah ini memerlukan analisis mendalam untuk memastikan data yang digunakan benar-benar berkualitas.

2.  **Data Integration (Integrasi Data)**\
    Sering kali, data berasal dari berbagai sumber, seperti **database yang berbeda, API, atau file CSV yang terpisah**. Integrasi data memerlukan waktu untuk menggabungkan sumber data yang berbeda dan menangani potensi **redundansi atau ketidaksesuaian antar variabel**.

3.  **Data Transformation (Transformasi Data)**\
    Data mentah harus diubah agar sesuai dengan algoritma yang digunakan. Contoh transformasi yang umum dilakukan adalah **normalisasi atau standarisasi** untuk memastikan bahwa semua variabel berada dalam skala yang sama. Selain itu, variabel kategorikal mungkin perlu dikonversi menjadi bentuk numerik melalui **one-hot encoding**.

4.  **Data Reduction (Reduksi Data)**\
    Jika dataset memiliki terlalu banyak variabel atau dimensi, perlu dilakukan reduksi menggunakan metode seperti **Principal Component Analysis (PCA)** atau **feature selection**. Reduksi ini penting untuk meningkatkan efisiensi analisis dan mengurangi noise yang dapat mengganggu hasil klasterisasi.

5.  **Data Discretization (Diskretisasi Data)**\
    Beberapa algoritma klaster lebih efektif jika data dalam bentuk kategori daripada numerik. Oleh karena itu, diskretisasi data sering kali diperlukan, baik melalui metode **equal-width binning, equal-frequency binning**, atau menggunakan pendekatan berbasis **pohon keputusan**.

**80% Waktu untuk Pemrosesan Data: Realitas dalam Analisis Data Mining**

Dalam praktiknya, penerapan algoritma klaster seperti **K-Means, Hierarchical Clustering, atau DBSCAN** relatif cepat dibandingkan dengan tahap pemrosesan data. Hal ini karena algoritma klaster bekerja optimal hanya jika data telah bersih dan siap digunakan. Jika pemrosesan data tidak dilakukan dengan baik, hasil klaster bisa menjadi **tidak akurat, sulit diinterpretasikan, atau bahkan tidak dapat digunakan** dalam pengambilan keputusan.

Oleh karena itu, dalam buku ini akan dibahas secara mendalam berbagai teknik **pemrosesan data awal menggunakan R**, termasuk paket dan fungsi yang dapat mempercepat proses ini, seperti **tidyverse, dplyr, tidyr, caret, dan FactoMineR**. Dengan pemahaman yang baik tentang tahap pemrosesan data, pembaca dapat meningkatkan efisiensi analisis klaster dan mendapatkan hasil yang lebih bermakna.

## Data Cleaning dan Data Reduction

Data yang tidak bersih, tidak lengkap, atau memiliki skala yang tidak seragam dapat menghasilkan klaster yang tidak akurat dan sulit diinterpretasikan. Oleh karena itu, tahap ini berperan penting dalam memastikan kualitas data sebelum analisis lebih lanjut dilakukan. Dalam analisis klaster, data menjadi bahan utama yang akan digunakan untuk membentuk kelompok-kelompok berdasarkan kemiripan antar data. Jika data yang digunakan memiliki banyak noise atau kesalahan, hasil klasterisasi dapat menjadi tidak valid atau bahkan menyesatkan. Oleh karena itu, pemrosesan data awal bertujuan untuk meningkatkan kualitas data agar analisis dapat dilakukan dengan lebih optimal.

Dalam analisis klaster, pemrosesan data awal adalah tahap yang sangat krusial untuk memastikan bahwa data yang digunakan memiliki kualitas yang baik dan dapat menghasilkan hasil klaster yang valid. Dua aspek utama dalam pemrosesan data awal adalah **data cleaning** dan **data reduction**. Data cleaning bertujuan untuk menangani ketidakkonsistenan dalam data, sedangkan data reduction membantu mengurangi dimensi data tanpa kehilangan informasi yang signifikan. Kedua proses ini diperlukan agar analisis klaster dapat dilakukan dengan lebih efisien dan menghasilkan kelompok yang lebih bermakna.

**Data cleaning** adalah proses membersihkan data dari kesalahan, duplikasi, nilai yang hilang, atau inkonsistensi lainnya. Data yang kotor dapat menyebabkan bias dalam analisis klaster, sehingga langkah ini harus dilakukan dengan cermat. Kesalahan umum dalam data meliputi entri yang salah, format yang tidak konsisten, atau data yang tidak lengkap. Dalam R, berbagai fungsi seperti `na.omit()`, `tidyverse::drop_na()`, dan `dplyr::mutate()` dapat digunakan untuk membersihkan data dengan menghapus atau mengganti nilai yang hilang dengan pendekatan yang sesuai.

Selain menangani nilai yang hilang, **data cleaning** juga mencakup normalisasi data untuk memastikan skala variabel yang seragam. Variabel dengan skala yang sangat berbeda dapat mendistorsi hasil klaster karena algoritma klaster berbasis jarak, seperti K-Means, sangat bergantung pada skala data. Teknik normalisasi seperti **min-max scaling** atau **z-score standardization** sering diterapkan menggunakan fungsi `scale()` dalam R. Dengan normalisasi, setiap variabel memiliki skala yang sebanding, sehingga proses klasterisasi lebih akurat.

Langkah berikutnya dalam data cleaning adalah menangani **outlier**, yaitu data yang memiliki nilai ekstrem yang tidak sesuai dengan pola umum. Outlier dapat mengganggu hasil klaster dengan menarik centroid ke arah yang tidak seharusnya. Identifikasi outlier dapat dilakukan dengan metode visual seperti boxplot (`ggplot2::geom_boxplot()`) atau menggunakan metode statistik seperti **IQR (Interquartile Range)**. Jika outlier ditemukan, opsi penanganannya meliputi penghapusan, transformasi, atau imputasi nilai berdasarkan distribusi data.

Setelah data dibersihkan, langkah selanjutnya dalam pemrosesan data awal adalah **data reduction**. Data reduction bertujuan untuk mengurangi dimensi data dengan tetap mempertahankan informasi yang paling relevan. Pengurangan dimensi ini sangat penting dalam analisis klaster karena terlalu banyak variabel dapat meningkatkan kompleksitas perhitungan dan menyebabkan fenomena **curse of dimensionality**, di mana data menjadi terlalu tersebar dan sulit untuk dikelompokkan secara efektif.

Salah satu metode umum dalam **data reduction** adalah **Principal Component Analysis (PCA)**, yang mengubah variabel asli menjadi sekumpulan variabel baru yang disebut **principal components**. PCA membantu merangkum informasi utama dalam beberapa komponen pertama, sehingga dimensi data dapat dikurangi tanpa kehilangan terlalu banyak informasi. Dalam R, PCA dapat dilakukan menggunakan fungsi `prcomp()` dari paket `stats` atau `PCA()` dari paket `FactoMineR`. Hasil PCA dapat divisualisasikan dengan `factoextra::fviz_pca_var()` untuk memahami kontribusi setiap variabel terhadap komponen utama.

Selain PCA, metode lain dalam **data reduction** adalah **feature selection**, di mana hanya variabel yang memiliki pengaruh signifikan yang dipertahankan untuk analisis klaster. Metode ini dapat dilakukan secara manual berdasarkan pemahaman domain, atau secara otomatis menggunakan teknik statistik seperti **Variance Thresholding**, **Correlation Analysis**, atau metode berbasis **Random Forest** dengan fungsi `randomForest::importance()`. Dengan memilih fitur yang relevan, analisis klaster dapat berjalan lebih cepat dan hasil yang diperoleh lebih interpretable.

Teknik lain dalam data reduction adalah **sampling**, yang bertujuan untuk mengurangi jumlah observasi sambil mempertahankan pola utama dalam data. Teknik ini berguna ketika data yang tersedia sangat besar dan tidak mungkin dianalisis secara langsung. Sampling dapat dilakukan secara acak (`sample()` dalam R) atau dengan mempertahankan proporsi karakteristik utama menggunakan metode **stratified sampling** (`dplyr::group_by()` dan `sample_n()`). Dengan teknik ini, analisis klaster dapat dilakukan lebih cepat tanpa kehilangan informasi utama dalam data.

Dengan menerapkan **data cleaning** dan **data reduction**, dataset yang digunakan dalam analisis klaster akan menjadi lebih bersih, terstruktur, dan lebih mudah untuk diproses oleh algoritma klasterisasi. Kedua langkah ini memastikan bahwa hasil klaster yang diperoleh lebih akurat dan dapat diinterpretasikan dengan lebih baik. Tanpa pemrosesan data awal yang tepat, hasil klaster dapat menjadi tidak valid atau sulit untuk digunakan dalam pengambilan keputusan. Oleh karena itu, pemahaman dan penerapan teknik ini dalam R sangat penting bagi siapa saja yang ingin melakukan analisis klaster yang berkualitas.

## Data Transformation dan Data Discretization

Dalam analisis klaster, pemrosesan data awal sangat penting untuk memastikan bahwa data siap digunakan dalam proses pengelompokan. Dua langkah utama dalam tahap ini adalah **data transformation** dan **data discretization**. **Data transformation** bertujuan untuk mengubah struktur atau distribusi data agar lebih sesuai dengan algoritma klaster, sementara **data discretization** mengonversi data numerik menjadi kategori untuk analisis yang lebih sederhana dan interpretasi yang lebih mudah. Kedua teknik ini berperan penting dalam meningkatkan kualitas klasterisasi dan menghindari bias akibat skala atau distribusi data yang tidak sesuai.

**Data transformation** adalah proses mengubah format atau skala data agar lebih sesuai dengan algoritma klasterisasi. Salah satu bentuk transformasi yang umum adalah **normalisasi**, yaitu proses menyamakan skala variabel sehingga tidak ada satu variabel yang mendominasi perhitungan jarak dalam algoritma berbasis jarak seperti **K-Means** atau **Hierarchical Clustering**. Normalisasi dapat dilakukan dengan metode **Min-Max Scaling** dan **Z-score Standardization**. Dalam R, normalisasi dapat diterapkan menggunakan fungsi `scale()` untuk standardisasi atau menggunakan paket seperti `caret::preProcess()` untuk berbagai metode transformasi.

Selain normalisasi, **transformasi logaritmik** sering digunakan untuk mengatasi distribusi data yang sangat miring (skewed). Data yang memiliki distribusi skewed dapat memengaruhi hasil klaster karena jarak antarobservasi menjadi tidak proporsional. Dengan menerapkan transformasi logaritmik (`log()` dalam R) atau transformasi Box-Cox (`MASS::boxcox()`), distribusi data dapat diperbaiki sehingga algoritma klaster dapat bekerja dengan lebih baik. Teknik ini sangat berguna dalam menangani variabel dengan rentang nilai yang luas atau data yang mengikuti distribusi Pareto.

Bentuk lain dari **data transformation** adalah **one-hot encoding**, yang digunakan untuk mengubah variabel kategorikal menjadi bentuk numerik yang dapat digunakan dalam algoritma klasterisasi. Sebagian besar algoritma klaster memerlukan data dalam bentuk numerik, sehingga variabel kategorikal harus dikonversi terlebih dahulu. Dalam R, one-hot encoding dapat dilakukan menggunakan fungsi `dummyVars()` dari paket `caret` atau dengan menggunakan `model.matrix()`. Teknik ini memastikan bahwa informasi dalam variabel kategorikal tetap dapat digunakan dalam analisis klaster.

Sementara itu, **data discretization** bertujuan untuk mengonversi data numerik menjadi kategori atau interval yang lebih mudah dianalisis. Teknik ini sering digunakan ketika data numerik terlalu rinci atau ketika analisis klaster ingin dilakukan berdasarkan kategori tertentu. Discretization dapat membantu dalam interpretasi hasil klaster dan mengurangi noise dalam data. Salah satu metode yang umum digunakan adalah **equal-width binning**, di mana rentang nilai dibagi menjadi beberapa interval dengan lebar yang sama menggunakan fungsi `cut()` dalam R.

Selain equal-width binning, metode **equal-frequency binning** juga sering digunakan, di mana setiap bin berisi jumlah observasi yang hampir sama. Teknik ini berguna ketika distribusi data tidak merata, sehingga setiap kelompok memiliki jumlah data yang seimbang. Dalam R, equal-frequency binning dapat dilakukan menggunakan `quantile()` untuk menentukan batas interval berdasarkan persentil data. Dengan metode ini, distribusi kategori yang dihasilkan lebih proporsional dibandingkan dengan equal-width binning.

Pendekatan lain dalam data discretization adalah **k-means discretization**, yang menggunakan algoritma K-Means untuk menemukan batas interval berdasarkan pola data. Teknik ini lebih adaptif dibandingkan equal-width atau equal-frequency binning karena mempertimbangkan distribusi alami data. Dalam R, metode ini dapat diterapkan dengan menjalankan K-Means (`kmeans()`) pada satu variabel dan menggunakan hasil klaster sebagai kategori baru. Teknik ini sangat berguna dalam data dengan distribusi kompleks yang tidak dapat ditangani dengan binning sederhana.

Selain metode berbasis interval, **decision tree-based discretization** juga menjadi alternatif yang efektif. Teknik ini menggunakan algoritma pohon keputusan seperti **CART** atau **C4.5** untuk menentukan batasan kategori berdasarkan aturan pemisahan yang optimal. Dalam R, pendekatan ini dapat dilakukan menggunakan paket `rpart` untuk membangun pohon keputusan dan kemudian mengekstrak batas kategorinya. Metode ini sering digunakan dalam aplikasi machine learning yang menggabungkan analisis klaster dengan model prediktif.

Dengan menerapkan **data transformation** dan **data discretization**, analisis klaster dapat dilakukan dengan lebih optimal, baik dalam aspek perhitungan maupun interpretasi hasil. Data transformation memastikan bahwa skala dan distribusi data tidak menghambat proses klasterisasi, sementara data discretization membantu dalam mengelompokkan data ke dalam kategori yang lebih mudah dianalisis. Oleh karena itu, pemahaman mendalam tentang kedua teknik ini sangat penting bagi siapa saja yang ingin melakukan analisis klaster yang lebih akurat dan interpretable menggunakan R.

## Dataset

Dataset yang digunakan dalam buku ini diperoleh dari **UCI Machine Learning Repository**, sebuah sumber dataset publik yang banyak digunakan dalam penelitian dan pembelajaran mesin. UCI Machine Learning Repository menyediakan berbagai dataset dari berbagai domain, seperti kesehatan, keuangan, pemasaran, dan sains sosial, yang telah digunakan secara luas dalam penelitian akademik maupun aplikasi industri.

Pemilihan dataset dari UCI didasarkan pada kriteria kualitas, keberagaman, serta relevansinya dalam analisis klaster. Dataset yang tersedia di repository ini umumnya telah melalui tahap kurasi dan dokumentasi yang baik, sehingga memudahkan pengguna dalam memahami struktur data dan variabel yang tersedia. Selain itu, dataset dalam repository ini mencakup berbagai ukuran dan kompleksitas, memungkinkan pembaca untuk mempelajari berbagai teknik analisis klaster dari skala kecil hingga besar.

Dalam buku ini, beberapa dataset yang digunakan mencakup data dengan variabel numerik, kategorikal, serta kombinasi keduanya, sehingga memberikan wawasan yang komprehensif tentang penerapan metode klasterisasi dalam berbagai jenis data. Setiap dataset akan dijelaskan secara rinci, termasuk deskripsi variabel, tujuan analisis, serta metode pemrosesan awal yang diperlukan sebelum diterapkan dalam algoritma klasterisasi.

Dengan menggunakan dataset dari UCI Machine Learning Repository, pembaca dapat dengan mudah mengakses dan mengunduh dataset yang digunakan dalam buku ini untuk bereksperimen secara mandiri menggunakan **R**. Dataset ini juga memungkinkan pembaca untuk menguji metode yang dipelajari dengan variasi parameter yang berbeda dan mengeksplorasi hasil klasterisasi secara lebih mendalam.
